\paragraph{Parallel SVM}
Parallel SVM algorithms can broadly be classified into explicit and implicit approaches. Explicit approaches identify  independent components of the problem and use parallel frameworks such as OpenMP, MPI, CUDA/OpenCL or MapReduce to directly encode this parallelism. Implicit approaches represent the calculations required for the SVM problem via matrix operations for which highly optimized parallel libraries exist. These approaches have their own advantages and disadvantages some of which we will discuss below. A detailed comparison and empirical analysis of popular explicit and implicit approaches on CPUs and GPUs is presented in \cite{tyree2014parallel}.

Explicit approaches require code for the problem to be written by hand. This usually involves significant engineering effort and requires fine-tuning of the tradeoffs between parallel work and framework overhead. The underlying algorithm for explicit approaches may be based on Platt's SMO algorithm~\citep{platt1999fast}, interior point approaches or gradient projection approaches. However, \cite{brugger2006parallel} and \cite{tyree2014parallel} note that competitive approaches in this vein are usually based on SMO and other similar dual decomposition procedures. SMO considers two variables and performs a block coordinate-descent w.r.t. these two variables. It is also possible to consider larger groups of variables for each iteration and various approaches have been proposed to split the SVM problem into subproblems which can be solved in parallel~\citep{cao2006parallel, collobert2002parallel, graf2004parallel, zhang2005parallel}. Of special interest to this paper is DC-SVM~\citep{hsieh2013divide}, which divides the data into subproblems based on a hierarchical clustering. \cite{hazan2008parallel} consider a decomposition based on Fenchel duality.  PSVM~\citep{zhu2008parallelizing} proposes a distributed algorithm for SVMs. \cite{zhao2011parallel} propose a parallel SVM algorithm in the MapReduce framework. GTSVM~\citep{cotter2011gpu} harness the specific capabilities of GPUs. 

In addition to the individual iterations, kernel computations make up a significant portion of the overall computation time and can also be parallelized. The multicore version of LibSVM~\citep{chang2011libsvm} includes only this parallelization with OpenMP and this results in a significant significant speedup.

Implicit parallelization approaches formulate the SVM problem as matrix-vector multiplications and use optimized parallel linear algebra libraries for the parallelism. \cite{sha2002multiplicative} propose a multiplicative update rule to solve SVMs, however directly implementing this method requires the (dense) kernel matrix to be stored explicity and is thus memory-intensive. \cite{chapelle2007training} present a multiplicative algorithm for the squared hinge loss via dense matrix operations. \cite{keerthi2006building} reduce the complexity of this approach by using a sparse basis set for the dual variables so that only part of the kernel matrix needs to be computed. This approach is known as Sparse Primal SVM (SP-SVM) and lends itself to easy implicit parallelization. \cite{tyree2014parallel} show that this method is very efficient for medium-scale datasets.

Our algorithm is also an explicit algorithm based on a no-bias version of the SVM problem which reduces to coordinate descent similar to that in \cite{hsieh2013divide}, and we use Galois (described below) as a parallel framework.

\paragraph{Galois}
Galois is a parallel framework based on the operator-schedule paradigm introduced in \cite{pingali2011tao}. A parallel Galois program is expressed via a parallel operator which operates on a node in a graph, and a schedule specifying the order in which nodes should be operated on. The schedule can be modified by the program as it executes and thus allows for \emph{data-dependent parallelism}. The operator is only allowed to access information for the node it is operating on and its neighbours, thus serializability can be maintained by allowing only non-adjacent nodes to be processed simultaneously.

For no-bias SVM solved with dual coordinate descent as in the present work, each data point corresponds to a node in the graph and edges in the graph indicate whether those nodes are independent according to Theorem~\ref{thm:serializabilty}. For coordinate descent, the choice of schedule does not affect convergence, but it is expected to affect the rate of convergence.